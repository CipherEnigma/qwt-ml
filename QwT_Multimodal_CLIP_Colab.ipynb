{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fef943",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.is_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(\"\\n‚úÖ GPU detected! Ready for quantization experiments.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: GPU not detected!\")\n",
    "    print(\"This code requires CUDA GPU. Check nvidia-smi and PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75aee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to QwT multimodal directory\n",
    "os.chdir('QwT-mm-RepQ-ViT')\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify essential files exist\n",
    "essential_files = ['main.py', 'mmm_ptq.py', 'qwerty.py', 'zero_shot.py']\n",
    "essential_dirs = ['models', 'quant', 'dataset', 'utils']\n",
    "\n",
    "print(\"\\nüìÅ Checking files:\")\n",
    "for file in essential_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(file) else \"‚ùå\"\n",
    "    print(f\"{exists} {file}\")\n",
    "\n",
    "print(\"\\nüìÇ Checking directories:\")\n",
    "for dir in essential_dirs:\n",
    "    exists = \"‚úÖ\" if os.path.isdir(dir) else \"‚ùå\"\n",
    "    print(f\"{exists} {dir}/\")\n",
    "\n",
    "print(\"\\n‚úÖ All files present!\" if all(os.path.exists(f) for f in essential_files) else \"‚ö†Ô∏è Missing files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8103d",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies (~5-10 minutes)\n",
    "\n",
    "**What this does:** Installs all required Python packages:\n",
    "- **PyTorch with CUDA** - Deep learning framework with GPU support\n",
    "- **open_clip_torch** - OpenAI CLIP implementation\n",
    "- **timm==0.4.12** - Timm vision models (specific version for compatibility)\n",
    "- **webdataset==0.2.100** - Efficient large-scale dataset loading\n",
    "- **ftfy, regex, tqdm** - Text processing and progress bars\n",
    "- **termcolor, scipy** - Utilities for output and linear algebra\n",
    "\n",
    "**Note:** If PyTorch is already installed with CUDA, skip the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce19de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA (uncomment if needed)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install CLIP and core dependencies\n",
    "!pip install ftfy regex tqdm\n",
    "!pip install open_clip_torch\n",
    "\n",
    "# Install specific versions for compatibility\n",
    "!pip install timm==0.4.12\n",
    "!pip install webdataset==0.2.100\n",
    "\n",
    "# Install utilities\n",
    "!pip install termcolor scipy\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd0209",
   "metadata": {},
   "source": [
    "## Step 4: Download CC3M Dataset for Calibration\n",
    "\n",
    "**What this does:** Downloads CC3M (Conceptual Captions 3M) dataset in WebDataset format.\n",
    "\n",
    "**Why CC3M specifically?**\n",
    "- **Real image-text pairs:** 3.3M natural images with human-written captions\n",
    "- **Better for CLIP:** Text encoder needs actual captions, not just class labels\n",
    "- **WebDataset format:** Efficient streaming from .tar shards (no need to extract all files)\n",
    "\n",
    "**What you need:**\n",
    "- Download 1-2 CC3M shards (.tar files) for calibration (512 samples)\n",
    "- Each shard is ~1GB and contains thousands of image-caption pairs\n",
    "\n",
    "**Format:** `cc3m-train-{0000..0575}.tar` (576 total shards available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CC3M WebDataset shards\n",
    "# Official instructions: https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md\n",
    "\n",
    "!mkdir -p ~/cc3m\n",
    "\n",
    "# Download first 2 shards (enough for calibration)\n",
    "# Each shard contains ~5,000-6,000 image-text pairs\n",
    "# We only need 512 samples for calibration, so 1-2 shards is plenty\n",
    "\n",
    "# Option 1: Download from Hugging Face (if available)\n",
    "!wget https://huggingface.co/datasets/conceptual_captions/cc3m-wds/resolve/main/cc3m-train-0000.tar -P ~/cc3m/\n",
    "!wget https://huggingface.co/datasets/conceptual_captions/cc3m-wds/resolve/main/cc3m-train-0001.tar -P ~/cc3m/\n",
    "\n",
    "# Option 2: If above doesn't work, download from img2dataset following their instructions\n",
    "# See: https://github.com/rom1504/img2dataset\n",
    "\n",
    "import os\n",
    "import glob\n",
    "shards = glob.glob(os.path.expanduser('~/cc3m/*.tar'))\n",
    "print(f\"\\n‚úÖ CC3M shards downloaded!\")\n",
    "print(f\"Shards: {len(shards)}\")\n",
    "print(f\"Location: ~/cc3m/\")\n",
    "print(f\"\\nThese will be used for calibration (512 image-text pairs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1cf658",
   "metadata": {},
   "source": [
    "## Step 5: Verify ImageNet Dataset\n",
    "\n",
    "**What this does:** Checks that your ImageNet-1K validation set is properly organized.\n",
    "\n",
    "**You already have ImageNet** - just verify the path and structure.\n",
    "\n",
    "**Required structure:**\n",
    "```\n",
    "/path/to/imagenet/val/\n",
    "  n01440764/\n",
    "    ILSVRC2012_val_00000293.JPEG\n",
    "    ...\n",
    "  n01443537/\n",
    "    ...\n",
    "  ... (1000 class folders total)\n",
    "```\n",
    "\n",
    "**Update the path below** to match your ImageNet location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ImageNet dataset location\n",
    "# UPDATE THIS PATH to where your ImageNet validation set is located\n",
    "IMAGENET_PATH = \"~/imagenet/val\"  # Change this to your actual path\n",
    "\n",
    "import os\n",
    "imagenet_path = os.path.expanduser(IMAGENET_PATH)\n",
    "\n",
    "if os.path.exists(imagenet_path):\n",
    "    num_classes = len([d for d in os.listdir(imagenet_path) if os.path.isdir(os.path.join(imagenet_path, d))])\n",
    "    \n",
    "    # Count total images\n",
    "    total_images = 0\n",
    "    for class_dir in os.listdir(imagenet_path):\n",
    "        class_path = os.path.join(imagenet_path, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            total_images += len([f for f in os.listdir(class_path) if f.endswith(('.JPEG', '.jpg', '.png'))])\n",
    "    \n",
    "    print(f\"‚úÖ ImageNet validation set found!\")\n",
    "    print(f\"Path: {imagenet_path}\")\n",
    "    print(f\"Classes: {num_classes}\")\n",
    "    print(f\"Total images: {total_images}\")\n",
    "    \n",
    "    if num_classes == 1000 and total_images == 50000:\n",
    "        print(\"\\n‚úÖ Complete ImageNet-1K validation set (1000 classes, 50K images)\")\n",
    "    elif num_classes == 1000:\n",
    "        print(f\"\\n‚úÖ ImageNet-1K with {num_classes} classes, {total_images} images\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Expected 1000 classes, found {num_classes}\")\n",
    "else:\n",
    "    print(f\"‚ùå ImageNet not found at: {imagenet_path}\")\n",
    "    print(\"Please update IMAGENET_PATH variable above to your actual ImageNet location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e593b1",
   "metadata": {},
   "source": [
    "## Step 6: Mode 1 - FP32 Baseline Evaluation\n",
    "\n",
    "**What this does:** Zero-shot ImageNet classification in full 32-bit precision (no quantization).\n",
    "\n",
    "**How it works:**\n",
    "1. Loads CLIP ViT-B/32 (~150M params)\n",
    "2. Encodes 1000 ImageNet class names as text embeddings\n",
    "3. Compares image embeddings with class embeddings\n",
    "4. Reports Top-1/Top-5 accuracy\n",
    "\n",
    "**Dataset usage:**\n",
    "- **Calibration:** Not needed for FP32 (only used in quantized modes)\n",
    "- **Evaluation:** Your ImageNet-1K validation set (1000 classes, 50K images)\n",
    "\n",
    "**Expected:** ~63.4% Top-1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision-only W6/A6 WITHOUT QwT (Baseline) ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT Baseline (W6/A6, Vision-only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice image_only \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 6 \\\n",
    "    --aq_params 6\n",
    "\n",
    "# Expected: ~59.2% Top-1 (4.2% drop from FP32)\n",
    "# Calibration uses 512 samples from CC3M WebDataset shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FP32 baseline evaluation\n",
    "!python main.py \\\n",
    "    --choice fp32_eval \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --batch-size 128\n",
    "\n",
    "# Expected output:\n",
    "# Top-1 accuracy: ~63.4%\n",
    "# Top-5 accuracy: ~86.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision-only W6/A6 WITH QwT (Improved) ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT + QwT (W6/A6, Vision-only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice image_only \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 6 \\\n",
    "    --aq_params 6 \\\n",
    "    --qwerty\n",
    "\n",
    "# Expected: ~60.3% Top-1 (+1.1% improvement over baseline!)\n",
    "# QwT adds lightweight compensation layers using CC3M calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd06d91",
   "metadata": {},
   "source": [
    "## Step 8: Mode 3 - Full Quantization (Vision + Text Encoders)\n",
    "\n",
    "**What this does:** Quantizes **BOTH vision and text encoders**. This is much more challenging but necessary for retrieval tasks or when text encoder runs frequently.\n",
    "\n",
    "**Why it's harder:** Text transformers are extremely sensitive to quantization. Small errors in attention scores cause catastrophic accuracy collapse.\n",
    "\n",
    "**Results show QwT's strength:**\n",
    "- **Without QwT:** W6/A6 drops to 29.8% (-33.6% catastrophic! üî¥)\n",
    "- **With QwT:** W6/A6 achieves 43.5% (-19.9% manageable) ‚úÖ **+13.7% massive recovery!**\n",
    "\n",
    "**How QwT helps:** Linear compensation layers specifically target sensitive layers in text encoder where quantization errors accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision+Text W6/A6 WITHOUT QwT (Shows the problem) ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT Baseline (W6/A6, Vision+Text)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice all_quant \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 6 \\\n",
    "    --aq_params 6\n",
    "\n",
    "# Expected: ~29.8% Top-1 (SEVERE 33.6% degradation!)\n",
    "# Text encoder quantization causes massive accuracy collapse\n",
    "# CC3M calibration data includes captions needed for text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision+Text W6/A6 WITH QwT (Massive recovery!) ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT + QwT (W6/A6, Vision+Text)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice all_quant \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 6 \\\n",
    "    --aq_params 6 \\\n",
    "    --qwerty\n",
    "\n",
    "# Expected: ~43.5% Top-1 (+13.7% improvement! üöÄ)\n",
    "# QwT's linear compensation rescues text encoder from quantization collapse\n",
    "# This is where QwT really shines - recovering from severe degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc9680",
   "metadata": {},
   "source": [
    "## Step 9: Higher Precision Experiments (W8/A8)\n",
    "\n",
    "**What this does:** Tests with 8-bit quantization (more bits = higher accuracy, less compression).\n",
    "\n",
    "**Use case:** When you need accuracy closer to FP32 but still want faster inference and smaller model size.\n",
    "\n",
    "**Expected results:**\n",
    "- **Vision-only W8/A8 + QwT:** 63.0% (nearly matches FP32!)\n",
    "- **Vision+Text W8/A8 + QwT:** 54.6% (still shows QwT's +15.9% improvement)\n",
    "\n",
    "**Trade-off:** W8/A8 uses 2√ó more memory than W4/A4, but still 4√ó less than FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision-only W8/A8 with QwT ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT + QwT (W8/A8, Vision-only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice image_only \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 8 \\\n",
    "    --aq_params 8 \\\n",
    "    --qwerty\n",
    "\n",
    "# Expected: ~63.0% Top-1 (only 0.4% below FP32!)\n",
    "# W8/A8 is almost lossless for vision-only quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22100841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vision+Text W8/A8 with QwT ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"RepQ-ViT + QwT (W8/A8, Vision+Text)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python main.py \\\n",
    "    --choice all_quant \\\n",
    "    --model \"ViT-B/32\" \\\n",
    "    --imagenet-val ~/imagenet/val \\\n",
    "    --train-data \"~/cc3m/cc3m-train-{0000..0001}.tar\" \\\n",
    "    --dataset-type webdataset \\\n",
    "    --batch-size 128 \\\n",
    "    --iter 4 \\\n",
    "    --wq_params 8 \\\n",
    "    --aq_params 8 \\\n",
    "    --qwerty\n",
    "\n",
    "# Expected: ~54.6% Top-1\n",
    "# Even at W8/A8, text encoder still benefits from QwT (+15.9% vs baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519ea8b",
   "metadata": {},
   "source": [
    "## Results Summary & Analysis\n",
    "\n",
    "**Complete experimental results table:**\n",
    "\n",
    "| Mode | Quantization | Method | Bits | Top-1 | Improvement |\n",
    "|------|-------------|--------|------|-------|-------------|\n",
    "| **Vision-only** | None | FP32 | 32/32 | 63.4% | - |\n",
    "| Vision-only | PTQ | RepQ-ViT | 6/6 | 59.2% | - |\n",
    "| Vision-only | PTQ + QwT | RepQ-ViT + QwT | 6/6 | **60.3%** | +1.1% |\n",
    "| Vision-only | PTQ + QwT | RepQ-ViT + QwT | 8/8 | **63.0%** | +0.1% |\n",
    "| **Vision+Text** | None | FP32 | 32/32 | 63.4% | - |\n",
    "| Vision+Text | PTQ | RepQ-ViT | 6/6 | 29.8% | - |\n",
    "| Vision+Text | PTQ + QwT | RepQ-ViT + QwT | 6/6 | **43.5%** | +13.7% üöÄ |\n",
    "| Vision+Text | PTQ | RepQ-ViT | 8/8 | 38.7% | - |\n",
    "| Vision+Text | PTQ + QwT | RepQ-ViT + QwT | 8/8 | **54.6%** | +15.9% üöÄ |\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "1. **Vision-only quantization is manageable** - RepQ-ViT baseline achieves reasonable accuracy even without QwT\n",
    "2. **Text encoder quantization is catastrophic** - Without QwT, accuracy drops 30%+ (unusable)\n",
    "3. **QwT shows massive gains for text encoders** - Recovers +13.7% at W6/A6, making it practical\n",
    "4. **Linear compensation is lightweight** - QwT adds <2% parameters but recovers significant accuracy\n",
    "5. **No training required** - Uses least-squares on just 512 calibration samples (no backprop!)\n",
    "6. **Lower bits = bigger QwT impact** - W4/A4 benefits more than W8/A8 from compensation\n",
    "\n",
    "## Why Text Encoder is Harder:\n",
    "- **Self-attention sensitivity:** Small quantization errors in Q/K/V matrices compound across 12 layers\n",
    "- **Narrow activation range:** Text tokens have less diversity than image patches\n",
    "- **Long-range dependencies:** Text attention spans full sequence, amplifying errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94786573",
   "metadata": {},
   "source": [
    "## Understanding the Implementation\n",
    "\n",
    "### Code Architecture:\n",
    "\n",
    "**1. `main.py`** - Entry point with argument parsing\n",
    "- Three modes: `fp32_eval`, `image_only`, `all_quant`\n",
    "- Loads model, datasets, calls MMM_PTQ class\n",
    "\n",
    "**2. `mmm_ptq.py`** - Core quantization logic (MMM = MultiModal Model)\n",
    "- `quantize_image_only_and_eval()` - Quantizes vision encoder only\n",
    "- `quantize_all_models_and_eval()` - Quantizes both encoders\n",
    "- `quant_ptq_1()` and `quant_ptq_2()` - Calibration routines\n",
    "- `qwerty()` and `qwerty_2()` - QwT compensation generation\n",
    "\n",
    "**3. `qwerty.py`** - QwT compensation layer implementation\n",
    "- `generate_compensation_model()` - Creates linear W, b parameters\n",
    "- Uses least-squares regression: solves `Ax = b` for compensation weights\n",
    "- Minimizes error between FP32 and quantized activations\n",
    "\n",
    "**4. `quant/quant_modules.py`** - Quantized layer implementations\n",
    "- `QuantConv2d` - Quantized 2D convolution\n",
    "- `QuantLinear` - Quantized fully-connected layer\n",
    "- `QuantMatMul` - Quantized matrix multiplication (for attention)\n",
    "\n",
    "**5. `quant/quantizer.py`** - Quantization functions\n",
    "- `UniformQuantizer` - Symmetric/asymmetric uniform quantization\n",
    "- Scale calculation: `scale = (max - min) / (2^n_bits - 1)`\n",
    "- Zero-point for asymmetric quantization\n",
    "\n",
    "**6. `dataset/dataset.py`** - Data loading\n",
    "- `get_wds_dataset()` - WebDataset loader for CC3M\n",
    "- `get_imagenet()` - ImageNet validation loader\n",
    "- Handles image preprocessing and text tokenization\n",
    "\n",
    "### QwT Algorithm Flow:\n",
    "\n",
    "```python\n",
    "# Step 1: Load pre-trained CLIP model\n",
    "model, preprocess, tokenizer = load_model(\"ViT-B/32\")\n",
    "\n",
    "# Step 2: Wrap model with quantization layers\n",
    "model = quant_model(model, weight_bits=6, activation_bits=6)\n",
    "\n",
    "# Step 3: Calibration - collect activation statistics\n",
    "for batch in calibration_dataloader:  # 512 samples\n",
    "    with torch.no_grad():\n",
    "        _ = model(batch)  # Forward pass updates quantizer stats\n",
    "\n",
    "# Step 4: Scale reparameterization (RepQ-ViT technique)\n",
    "scale_reparameterization(model)\n",
    "# Absorbs batch norm, fuses scales into weights\n",
    "\n",
    "# Step 5: Generate QwT compensation (if --qwerty flag set)\n",
    "if args.qwerty:\n",
    "    for block in model.blocks:\n",
    "        # Collect FP32 vs quantized activations\n",
    "        fp32_output = fp32_block(calibration_data)\n",
    "        quant_output = quant_block(calibration_data)\n",
    "        error = fp32_output - quant_output\n",
    "        \n",
    "        # Solve least-squares: W * quant_output + b ‚âà fp32_output\n",
    "        W, b = linear_regression(quant_output, fp32_output)\n",
    "        \n",
    "        # Attach compensation to next block\n",
    "        block.compensation = CompensationBlock(W, b)\n",
    "\n",
    "# Step 6: Evaluate on ImageNet\n",
    "accuracy = zero_shot_classification(model, imagenet_val)\n",
    "```\n",
    "\n",
    "### What Makes QwT Lightweight:\n",
    "\n",
    "- **No backpropagation:** Uses closed-form least-squares solution\n",
    "- **Tiny parameters:** Only adds linear layers (W, b) ~2% overhead\n",
    "- **Fast calibration:** 512 samples takes <1 minute\n",
    "- **Post-training:** Works on any pre-trained model without retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd9058",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@InProceedings{Fu_2025_CVPR,\n",
    "    author    = {Fu, Minghao and Yu, Hao and Shao, Jie and Zhou, Junjie and Zhu, Ke and Wu, Jianxin},\n",
    "    title     = {Quantization without Tears},\n",
    "    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "    year      = {2025},\n",
    "    pages     = {4462-4472}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
